{
  "cells": [
    {
      "metadata": {
        "id": "930bcd6c13654983"
      },
      "cell_type": "markdown",
      "source": [
        "# Predicting german doctor ratings with a BERT model\n",
        "## Fine tuning of a pretrained **Hugging Face** transfomer\n",
        "In this notebook we will be looking at the fine-tuning process of a BERT model that was previously pre-trained on a large german text corpus.\n",
        "\n",
        "You will build a classifier to predict doctor ratings from patients' text comments. Along the way you will load the dataset in the approprate format for a transformer, perform a perliminary data analysis, implement the neccessary layers in PyTorch and finally train the model.\n",
        "\n",
        "**Important**: Later you will use Tensorboard to track the training progress. Sometimes it can't properly be loaded if you are employing an adblocker. So better turn it off before you get started.\n",
        "\n",
        "A detailed description of the **German language reviews of doctors by patients 2019** dataset can be found [here](https://data.world/mc51/german-language-reviews-of-doctors-by-patients)\n",
        "\n",
        "\n",
        "For the feature creation and the modeling, we will use the [**Hugging Face**](https://huggingface.co/) implementation of transformers for PyTorch.\n",
        "\n",
        "Credits: This notebook and its implementation is heavily influenced by the [data-drive](https://data-dive.com/) *Natural Language Processing of German texts* blog post"
      ],
      "id": "930bcd6c13654983"
    },
    {
      "metadata": {
        "id": "e7c5187b4e7ab6ae"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "!pip install -U transformers==4.42.4"
      ],
      "id": "e7c5187b4e7ab6ae"
    },
    {
      "metadata": {
        "id": "c2b8ecf24b7ec81d"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import re\n",
        "import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tqdm.tqdm.pandas()\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "pd.options.display.max_colwidth = 600\n",
        "pd.options.display.max_rows = 400\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'{device=}')"
      ],
      "id": "c2b8ecf24b7ec81d"
    },
    {
      "metadata": {
        "id": "9e467fa81ed026d6"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the **German language reviews of doctors by patients 2019** dataset"
      ],
      "id": "9e467fa81ed026d6"
    },
    {
      "metadata": {
        "id": "bc1bbf1fd412480a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "!wget -O reviews.zip https://query.data.world/s/v5xl53bs2rgq476vqy7cg7xx2db55y\n",
        "!unzip reviews.zip"
      ],
      "id": "bc1bbf1fd412480a"
    },
    {
      "metadata": {
        "id": "c14e165e7f85f832"
      },
      "cell_type": "markdown",
      "source": [
        "### Data loading and cleaning\n",
        "The dataset contains patients' text comments in german and a coresponding rating ranging from 1-6 (from good to bad).\n",
        "We will turn the rating into a binary grade:\n",
        "- ratings from 1 to 2 will be considered \"good\" grades (0)\n",
        "- ratings from 5 to 6 will be considered \"bad\" grades (1)\n",
        "\n",
        "Entries with ratings inbetween and reviews shorter than 20 characters will be discarded.\n",
        "\n",
        "Then we perform some rudimentary clean up by removing taggs and special characters"
      ],
      "id": "c14e165e7f85f832"
    },
    {
      "metadata": {
        "id": "e21356cc5182775b"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        - remove any html tags (< /br> often found)\n",
        "        - Keep only ASCII + European Chars and whitespace, no digits\n",
        "        - remove single letter chars\n",
        "        - convert all whitespaces (tabs etc.) to single wspace\n",
        "    \"\"\"\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "    return text"
      ],
      "id": "e21356cc5182775b"
    },
    {
      "metadata": {
        "id": "190006ac5c4f721"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# read data from csv\n",
        "reviews = pd.read_csv(\"german_doctor_reviews.csv\")\n",
        "\n",
        "# Create binary grade, class 1-2 or 5-6  = good or bad\n",
        "reviews[\"grade\"] = np.nan\n",
        "reviews.loc[reviews[\"rating\"] <= 2, \"grade\"] = 0\n",
        "reviews.loc[reviews[\"rating\"] >= 5, \"grade\"] = 1\n",
        "\n",
        "reviews = reviews[reviews[\"comment\"].str.len() > 20]\n",
        "reviews = reviews.dropna(axis=\"index\", subset=[\"grade\"])\n",
        "\n",
        "reviews = reviews[[\"comment\", \"grade\"]]\n",
        "\n",
        "reviews[\"comment\"] = reviews[\"comment\"].progress_map(clean_text)"
      ],
      "id": "190006ac5c4f721"
    },
    {
      "metadata": {
        "id": "4d456a8d1709f262"
      },
      "cell_type": "markdown",
      "source": [
        "### Limiting the dataset size\n",
        "Here we limit the size of the dataset to accelerate the training iterations. Once everything is correctly set up we should traing on the entire dataset"
      ],
      "id": "4d456a8d1709f262"
    },
    {
      "metadata": {
        "id": "ddf7257fbaa0fe80"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "reviews = reviews.iloc[:10_000]"
      ],
      "id": "ddf7257fbaa0fe80"
    },
    {
      "metadata": {
        "id": "30bbc64c299d57b6"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset analysis\n",
        "\n",
        "##### Text inspection\n",
        "We successfully loaded the dataset into memory and performed some modification(and temporarly reduced its size ).\n",
        "Now we should ensure that the data is well structured and suitable for our training\n",
        "\n",
        "Have the grades correctly be assigned to the comments? Are there some outliers that do not make sense?"
      ],
      "id": "30bbc64c299d57b6"
    },
    {
      "metadata": {
        "id": "ccbc1084069c5c14"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "reviews.head(5)"
      ],
      "id": "ccbc1084069c5c14"
    },
    {
      "metadata": {
        "id": "1a320d2df8132361"
      },
      "cell_type": "markdown",
      "source": [
        "How does the distribution of the text length look like? How much of the comments will be truncated if the supported sequence length of the model is shorter?\n",
        "\n",
        "A good way to quickly get a grasp of the distribution is to visuzalize it as bars in a histogram plot\n",
        "\n",
        "**NOTE**: The data is loaded into a Pandas DataFrame. Use the built-in functionality compute aggregates over the 'text' column and directly plot it"
      ],
      "id": "1a320d2df8132361"
    },
    {
      "metadata": {
        "id": "4430dba73771d6cb"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "##########################\n",
        "## YOUR CODE HERE START ##\n",
        "##########################\n",
        "\n",
        "# compute the distribution over the text lengths in the dataset\n",
        "# plot the aggregation as histogram-bar-plot\n",
        "\n",
        "reviews['comment']....\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE END ##\n",
        "##########################"
      ],
      "id": "4430dba73771d6cb"
    },
    {
      "metadata": {
        "id": "c7552ba53e684869"
      },
      "cell_type": "markdown",
      "source": [
        "##### Label inspection\n",
        "\n",
        "Use the text data analysis techniques that you have learned so far to get a better feeling of our data.\n",
        "How does the distribution of grade look like, now that we have derived them from the reviewer rating?\n",
        "\n",
        "The best way to quickly get a grasp of the distribution is to visuzalize it as a bar-plot or piec-chart\n",
        "\n",
        "**NOTE**: The data is loaded into a Pandas DataFrame. Use the built-in functionality compute aggregates over the 'grade' column and directly plot it.\n",
        "\n",
        "Is the data distribution skewed? If so, why and how could this influence the training procedure?\n"
      ],
      "id": "c7552ba53e684869"
    },
    {
      "metadata": {
        "id": "2fd6995b02adfed0"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "##########################\n",
        "## YOUR CODE HERE START ##\n",
        "##########################\n",
        "\n",
        "# compute the distribution over the grades in the dataset\n",
        "# plot the aggregation as bar-plot\n",
        "\n",
        "reviews['grade']....\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE END ##\n",
        "##########################"
      ],
      "id": "2fd6995b02adfed0"
    },
    {
      "metadata": {
        "id": "35eccd2ddfd00a39"
      },
      "cell_type": "markdown",
      "source": [
        "## Setting up the encoder model\n",
        "HuggingFace's transfomer library provides pre-configured text tokenizers and pre-trained models that can be convininetly loaded either from local resources or from [HunggingFace's model hub](https://huggingface.co/models)\n",
        "\n",
        "Here we will be using the [**\"bert-base-german-cased\"**](https://huggingface.co/bert-base-german-cased) model that was pretrained on German Wikipedia dump (6GB of raw txt files), the OpenLegalData dump (2.4 GB) and news articles (3.6 GB)."
      ],
      "id": "35eccd2ddfd00a39"
    },
    {
      "metadata": {
        "id": "3432c94456099e5"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "id": "3432c94456099e5"
    },
    {
      "metadata": {
        "id": "ebfd86b6e780110a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")"
      ],
      "id": "ebfd86b6e780110a"
    },
    {
      "metadata": {
        "id": "7f43098af4129970"
      },
      "cell_type": "markdown",
      "source": [
        "### Text tokenization\n",
        "Here we tokenize each text review into input_ids: This corresponds to the tokens in the inputs converted into IDs.\n",
        "\n",
        "The tokenizer takes care of\n",
        "- extending the token sequence with the special tokens [CLS] and [SEP]\n",
        "- padding shorter sequences to the required length\n",
        "- truncating long sequences to a max length\n"
      ],
      "id": "7f43098af4129970"
    },
    {
      "metadata": {
        "id": "cf6f4686f34b696b"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "MAXLEN = 192\n",
        "\n",
        "def tokenize(review):\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        text=review,\n",
        "        add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
        "        max_length=MAXLEN,  # Max length to truncate/pad\n",
        "        padding='max_length',  # Pad sentence to max length\n",
        "        return_attention_mask=True,  # Return attention mask\n",
        "        return_token_type_ids=False,\n",
        "        truncation=True, )\n",
        "\n",
        "    return encoded['input_ids'], encoded['attention_mask']"
      ],
      "id": "cf6f4686f34b696b"
    },
    {
      "metadata": {
        "id": "d4653c69a9848c6c"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Tokenize all reviews\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for review in tqdm.tqdm(reviews['comment']):\n",
        "    ids, mask = tokenize(review)\n",
        "    input_ids.append(ids)\n",
        "    attention_masks.append(mask)\n",
        "\n",
        "input_ids = np.array(input_ids)\n",
        "attention_masks = np.array(attention_masks)"
      ],
      "id": "d4653c69a9848c6c"
    },
    {
      "metadata": {
        "id": "72c1c5f388496c82"
      },
      "cell_type": "markdown",
      "source": [
        "Now lets split the data into two datasets: one for training and one for testing.\n",
        "\n",
        "We will use 75% for training and 25% for testing"
      ],
      "id": "72c1c5f388496c82"
    },
    {
      "metadata": {
        "id": "d466d88ad1a7aa66"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "train_ids, test_ids, train_masks, test_masks, train_labels, test_labels = train_test_split(\n",
        "    input_ids,\n",
        "    attention_masks,\n",
        "    reviews[\"grade\"],\n",
        "    random_state=1,\n",
        "    test_size=0.25,\n",
        "    shuffle=True\n",
        ")\n",
        "print(f\"Train set: {len(train_ids)}\\nTest set: {len(test_ids)}\")"
      ],
      "id": "d466d88ad1a7aa66"
    },
    {
      "metadata": {
        "id": "7c492d72c077217"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Convert to PyTorch tensors\n",
        "train_ids = torch.tensor(train_ids, dtype=torch.long)\n",
        "test_ids = torch.tensor(test_ids, dtype=torch.long)\n",
        "train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
        "test_masks = torch.tensor(test_masks, dtype=torch.long)\n",
        "train_labels = torch.tensor(train_labels.values, dtype=torch.float)\n",
        "test_labels = torch.tensor(test_labels.values, dtype=torch.float)\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_dataset = TensorDataset(train_ids, train_masks, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_masks, test_labels)\n",
        "\n",
        "# Define hyperparameters\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "id": "7c492d72c077217"
    },
    {
      "metadata": {
        "id": "496ba870888d8def"
      },
      "cell_type": "markdown",
      "source": [
        "## Model creation\n",
        "Here we build our model using a pre-trained version of a BERT model.\n",
        "We will be using the output of encoder and input into a simple FFNN-stack. The last layer will translate the hidden states into a binary signal (class-probabilities bewtween 0 and 1).\n",
        "\n",
        "The documentation of the employed [**BertModel**](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel) class describes the returned output of the BERT model.\n",
        "\n",
        "**NOTE**:\n",
        "- the special token, [CLS], is appended to the start of the inputs. [CLS] stands for classifier token. The embedding of this token is summarizing the entire input i.e. the input-sequence\n",
        "- in the case of a classification problem, new additional layers on top of the BERT model would use this [CLS] token.\n",
        "- the `BertModel` returns an output that contains: `last_hidden_state` and `pooler_output`\n",
        " - `last_hidden_state` is a tensor that contains the embeddings of all output tokens i.e. for the [CLS] and all sequence token\n",
        " - `pooler_output` is a tensor that only contains the embedding of the [CLS]\n",
        " - use the `pooler_output` returned by the BertModel for the subsequent layers\n",
        "\n",
        "\n",
        "**NOTE**: This is a binary classification problem. Think about the correct activation function for this kind of task."
      ],
      "id": "496ba870888d8def"
    },
    {
      "metadata": {
        "id": "5010a12add5588e3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "##########################\n",
        "## YOUR CODE HERE START ##\n",
        "##########################\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-german-cased\")\n",
        "\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "\n",
        "        # Create a linear layer that maps from the embedding dimension to the output dimension\n",
        "        self.classifier = ....\n",
        "\n",
        "        # Create an activation layer with the appropriate function\n",
        "        self.sigmoid = ....\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # pass the cls_embedding to the classifier layer\n",
        "        x = ....\n",
        "\n",
        "        # pass the result to the activation layer\n",
        "        output = ....\n",
        "\n",
        "        return output\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE END ##\n",
        "##########################\n"
      ],
      "id": "5010a12add5588e3"
    },
    {
      "metadata": {
        "id": "f1a926fcfa3f5b06"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Initialize model\n",
        "model = BertClassifier()\n",
        "model.to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(model)"
      ],
      "id": "f1a926fcfa3f5b06"
    },
    {
      "metadata": {
        "id": "a3a1aaf0e155b414"
      },
      "cell_type": "markdown",
      "source": [
        "#### Finalizing the model\n",
        "Now that we successfully created our model, we have to finalize it by defining the loss that has to be minimized and which optimizier we want to use.\n",
        "\n",
        "The PyTorch documentation lists all available [losses](https://docs.pytorch.org/docs/stable/nn.html#loss-functions) and [optimizers](https://docs.pytorch.org/docs/stable/optim.html) you should choose from.\n",
        "\n",
        "**NOTE**: This is a binary classification problem. Think about the correct loss function for this kind of problem.\n",
        "\n",
        "**NOTE**: Make sure to initialize your optimizer with the learning rate that was defined above."
      ],
      "id": "a3a1aaf0e155b414"
    },
    {
      "metadata": {
        "id": "d299c85c6ea301a3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Define hyperparameters\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-5\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE START ##\n",
        "##########################\n",
        "\n",
        "# Create an instance of an Adam optimizer with the learning rate defined above\n",
        "# Define the loss to be minimized in this binary classification problem\n",
        "\n",
        "optimizer = ....\n",
        "criterion = ....\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE END ##\n",
        "##########################"
      ],
      "id": "d299c85c6ea301a3"
    },
    {
      "metadata": {
        "id": "222f2ada25bc67b9"
      },
      "cell_type": "markdown",
      "source": [
        "## Model training\n",
        "Now we are ready to fine-tune the BERT model on our dataset\n",
        "We will be monitoring the training process using tensorboard. Once the training is launched you will be able to inspect logged metrics and track the progress."
      ],
      "id": "222f2ada25bc67b9"
    },
    {
      "metadata": {
        "id": "6e1f3a9dc2fd6acb"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "id": "6e1f3a9dc2fd6acb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch in tqdm.tqdm(dataloader):\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predictions = (outputs > 0.5).float()\n",
        "        correct_predictions += (predictions == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(dataloader), correct_predictions / total_predictions\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device).unsqueeze(1)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = (outputs > 0.5).float()\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    return total_loss / len(dataloader), correct_predictions / total_predictions\n"
      ],
      "metadata": {
        "id": "tCJYkGLWVwuf"
      },
      "id": "tCJYkGLWVwuf",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fa2feec56db118c7"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "history = {'epoch': [], 'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "writer = SummaryWriter(f'logs/{datetime.now()}')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
        "\n",
        "    # Evaluate\n",
        "    val_loss, val_acc = evaluate(model, test_dataloader, criterion, device)\n",
        "\n",
        "    # Log metrics\n",
        "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/validation', val_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
        "    writer.add_scalar('Accuracy/validation', val_acc, epoch)\n",
        "\n",
        "    # Save history\n",
        "    history['epoch'].append(epoch)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        print(\"Saved best model!\")\n",
        "\n",
        "    print('-' * 60)"
      ],
      "id": "fa2feec56db118c7"
    },
    {
      "metadata": {
        "id": "5d87c8a1290d4bb7"
      },
      "cell_type": "markdown",
      "source": [
        "### Training metrics\n",
        "Lets inspect the training history to see how the losses and metrics behaved during training"
      ],
      "id": "5d87c8a1290d4bb7"
    },
    {
      "metadata": {
        "id": "301421abdb90ce7a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "history_df = pd.DataFrame(history).set_index('epoch')\n",
        "history_df"
      ],
      "id": "301421abdb90ce7a"
    },
    {
      "metadata": {
        "id": "7c618c974292aa96"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "history_df.plot()"
      ],
      "id": "7c618c974292aa96"
    },
    {
      "metadata": {
        "id": "b61dc9b43d91ac28"
      },
      "cell_type": "markdown",
      "source": [
        "## Model evaluation\n",
        "Lets have a look how well the model is preforming on our test dataset."
      ],
      "id": "b61dc9b43d91ac28"
    },
    {
      "metadata": {
        "id": "ba08a4ca163a1a4b"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "model.eval()\n",
        "\n",
        "# Get predictions\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        predictions = (outputs > 0.5).float().squeeze().cpu().numpy()\n",
        "\n",
        "        all_predictions.extend(predictions)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Generate classification report\n",
        "report = metrics.classification_report(y_true=all_labels,\n",
        "                                      y_pred=all_predictions)\n",
        "print(report)"
      ],
      "id": "ba08a4ca163a1a4b"
    },
    {
      "metadata": {
        "id": "588a8cd36ee6c8f9"
      },
      "cell_type": "markdown",
      "source": [
        "## Congratulation\n",
        "You have trained and evaluated a text classifier based on a pre-trained BERT transformer using PyTorch!"
      ],
      "id": "588a8cd36ee6c8f9"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
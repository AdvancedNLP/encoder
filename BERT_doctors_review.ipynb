{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "BERT_doctors_review.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeRF6YRIs9vt"
   },
   "source": [
    "# Predicting german doctor ratings with a BERT model\n",
    "## Fine tuning of a pretrained **Hugging Face** transfomer\n",
    "In this notebook we will be looking at the fine-tuning process of a BERT model that was previously pre-trained on a large german text corpus. We aim at building a classifier to predict doctor ratings from patients' text comments.\n",
    "\n",
    "A detailed description of the **German language reviews of doctors by patients 2019** dataset can be found [here](https://data.world/mc51/german-language-reviews-of-doctors-by-patients)\n",
    "\n",
    "\n",
    "For the feature creation and the modeling, we will use the [**Hugging Face**](https://huggingface.co/) implementation of transformers for Tensorflow 2.0. Transformers provides a general architecture implementation for several state of the art models in the natural language domain.\n",
    "\n",
    "NOTE: This notebook and its implementation is heavily influenced by the [data-drive](https://data-dive.com/) *Natural Language Processing of German texts* blog post"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DkqTxWLHouAC"
   },
   "source": [
    "!pip install -U transformers==4.9.2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s_fTsD6Lp49I"
   },
   "source": [
    "import re\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tqdm.tqdm.pandas()\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "pd.options.display.max_colwidth = 600\n",
    "pd.options.display.max_rows = 400"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyJ8aUbVwmmf"
   },
   "source": [
    "## Preparing the **German language reviews of doctors by patients 2019** dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kWlp3Yc0qlYZ"
   },
   "source": [
    "!wget -O reviews.zip https://query.data.world/s/v5xl53bs2rgq476vqy7cg7xx2db55y\n",
    "!unzip reviews.zip"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLPyfZ4Cwu2s"
   },
   "source": [
    "### Data loading and cleaning\n",
    "The dataset contains patients' text comments in german and a coresponding rating ranging from 1-6 (from good to bad).\n",
    "We will turn the rating into a binary grade:\n",
    "- ratings from 1 to 2 will be considered \"good\" grades (0)\n",
    "- ratings from 5 to 6 will be considered \"bad\" grades (1)\n",
    "\n",
    "Entries with ratings inbetween and reviews shorter than 20 characters will be discarded.\n",
    "\n",
    "Then we perform some rudimentary clean up by removing taggs and special characters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ihiqKf55ppcL"
   },
   "source": [
    "RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "    return text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LmdC34SJqkiY"
   },
   "source": [
    "# read data from csv\n",
    "reviews = pd.read_csv(\"german_doctor_reviews.csv\")\n",
    "\n",
    "# Create binary grade, class 1-2 or 5-6  = good or bad\n",
    "reviews[\"grade\"] = np.nan\n",
    "reviews.loc[reviews[\"rating\"] <= 2, \"grade\"] = 0\n",
    "reviews.loc[reviews[\"rating\"] >= 5, \"grade\"] = 1\n",
    "\n",
    "reviews = reviews[reviews[\"comment\"].str.len() > 20]\n",
    "reviews = reviews.dropna(axis=\"index\", subset=[\"grade\"])\n",
    "\n",
    "reviews = reviews[[\"comment\", \"grade\"]]\n",
    "\n",
    "reviews[\"comment\"] = reviews[\"comment\"].progress_map(clean_text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUq1NAly6PsY"
   },
   "source": [
    "### Limiting the dataset size\n",
    "Here we limit the size of the dataset to accelerate the training iterations. Once everything is correctly set up we should traing on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "syhMGDQLxG20"
   },
   "source": [
    "reviews = reviews.iloc[:10_000]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrQoV8W0-LG6"
   },
   "source": [
    "### Dataset analysis\n",
    "\n",
    "##### Text inspection\n",
    "We successfully loaded the dataset into memory and performed some modification(and temporarly reduced its size ).\n",
    "Now we should ensure that the data is well structured and suitable for our training\n",
    "\n",
    "Have the grades correctly be assigned to the comments? Are there some outliers that do not make sense?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FOhS2ra699hh"
   },
   "source": [
    "reviews.head(5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "How does the distribution of the text length look like? How much of the comments will be truncated if the supported sequence length of the model is shorter?\n",
    "\n",
    "A good way to quickly get a grasp of the distribution is to visuzalize it as bars in a histogram plot\n",
    "\n",
    "**NOTE**: The data is loaded into a Pandas DataFrame. Use the built-in functionality compute aggregates over the 'text' column and directly plot it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##########################\n",
    "## YOUR CODE HERE START ##\n",
    "##########################\n",
    "\n",
    "# compute the distribution over the text lengths in the dataset\n",
    "# plot the aggregation as histogram-bar-plot\n",
    "\n",
    "reviews['comment']....\n",
    "\n",
    "##########################\n",
    "## YOUR CODE HERE END ##\n",
    "##########################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Label inspection\n",
    "\n",
    "Use the text data analysis techniques that you have learned so far to get a better feeling of our data.\n",
    "How does the distribution of grade look like, now that we have derived them from the reviewer rating?\n",
    "\n",
    "The best way to quickly get a grasp of the distribution is to visuzalize it as a bar-plot or piec-chart\n",
    "\n",
    "**NOTE**: The data is loaded into a Pandas DataFrame. Use the built-in functionality compute aggregates over the 'grade' column and directly plot it.\n",
    "\n",
    "Is the data distribution skewed? If so, why and how could this influence the training procedure?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##########################\n",
    "## YOUR CODE HERE START ##\n",
    "##########################\n",
    "\n",
    "# compute the distribution over the grades in the dataset\n",
    "# plot the aggregation as bar-plot\n",
    "\n",
    "reviews['grade']....\n",
    "\n",
    "##########################\n",
    "## YOUR CODE HERE END ##\n",
    "##########################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up the encoder model\n",
    "HuggingFace's transfomer library provides pre-configured text tokenizers and pre-trained models that can be convininetly loaded either from local resources or from [HunggingFace's model hub](https://huggingface.co/models)\n",
    "\n",
    "Here we will be using the [**\"bert-base-german-cased\"**](https://huggingface.co/bert-base-german-cased) model that was pretrained on German Wikipedia dump (6GB of raw txt files), the OpenLegalData dump (2.4 GB) and news articles (3.6 GB)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text tokenization\n",
    "Here we tokenize each text review into input_ids: This corresponds to the tokens in the inputs converted into IDs. \n",
    "\n",
    "The tokenizer takes care of \n",
    "- extending the token sequence with the special tokens [CLS] and [SEP]\n",
    "- padding shorter sequences to the required length\n",
    "- truncating long sequences to a max length\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAXLEN = 192\n",
    "\n",
    "def tokenize(review):\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text=review,\n",
    "        add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "        max_length=MAXLEN,  # Max length to truncate/pad\n",
    "        padding='max_length',  # Pad sentence to max length\n",
    "        return_attention_mask=False,  # attention mask not needed for our task\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True, )\n",
    "      \n",
    "    return encoded['input_ids']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids = np.array([tokenize(review) for review in tqdm.tqdm(reviews['comment'])])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets split the data into two datasets: one for training and one for testing.\n",
    "\n",
    "We will use 75% for training and 25% for testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ids, test_ids, train_labels, test_labels = train_test_split(\n",
    "    input_ids, \n",
    "    reviews[\"grade\"], \n",
    "    random_state=1, \n",
    "    test_size=0.25, \n",
    "    shuffle=True\n",
    ")\n",
    "print(f\"Train set: {len(train_ids)}\\nTest set: {len(test_ids)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 8\n",
    "LEARNING_RATE = 1e-5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = (tf.data.Dataset.from_tensor_slices((train_ids, train_labels))\n",
    "                    .shuffle(buffer_size=len(train_ids), reshuffle_each_iteration=True)\n",
    "                    .repeat(EPOCHS)\n",
    "                    .batch(BATCH_SIZE))\n",
    "\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices((test_ids, test_labels))\n",
    "                    .batch(BATCH_SIZE))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model creation\n",
    "Here we build our model using a pre-trained version of a BERT model.\n",
    "We will be using the output of encoder and input into a simple FFNN-stack. The last layer will translate the hidden states into a binary signal (class-probabilities bewtween 0 and 1).\n",
    "\n",
    "The documentation of the employed [**TFBertModel**](https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertModel) class describes the returned output of the BERT model.\n",
    "\n",
    "**NOTE**:\n",
    "- the special token, [CLS], is appended to the start of the inputs. [CLS] stands for classifier token. The embedding of this token is summarizing the entire input i.e. the input-sequence\n",
    "- in the case of a classification problem, new additional layers on top of the BERT model would use this [CLS] token.\n",
    "- the `TFBertModel` returns to outputs: `last_hidden_state` and `pooler_output`\n",
    " - `last_hidden_state` is a tensor that contains the embeddings of all output tokens i.e. for the [CLS] and all sequence token\n",
    " - `pooler_output` is a tensor that only contains the embedding of the [CLS]\n",
    " - you can build your model either on the correct `last_hidden_state` or on the `pooler_output` returned by the TFBertModel implementation.\n",
    "\n",
    "**NOTE**: This is a binary classification problem. Think about the correct activation function for this kind of task."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_model(max_len=MAXLEN):\n",
    "    \"\"\" add binary classification to pretrained model\n",
    "    \"\"\"\n",
    "\n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\"\n",
    "    )\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-german-cased\")\n",
    "    encoder_outputs = bert_model(input_word_ids)\n",
    "\n",
    "    ##########################\n",
    "    ## YOUR CODE HERE START ##\n",
    "    ##########################\n",
    "    \n",
    "    # build a binary classification stack \n",
    "    # on top of the sequence embeddings\n",
    "\n",
    "    # Either use last_hidden_state use pooler_output \n",
    "    # that were returned in encoder_outputs\n",
    "    \n",
    "    cls_embedding = ....\n",
    "\n",
    "\n",
    "    # Now add a FFNN that takes in the cls_embeddings and outputs a structure suitable\n",
    "    # for a binary classification \n",
    "\n",
    "    # Create a layer that maps from the embedding dimension to the output dimension\n",
    "    # and pass in the cls_embedding\n",
    "    stack = ....\n",
    "\n",
    "    # Create an activation layer with the appropriate function and pass in the\n",
    "    # NN-stack\n",
    "    output = ....\n",
    "\n",
    "    ##########################\n",
    "    ## YOUR CODE HERE END ##\n",
    "    ##########################\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input_word_ids, outputs=output)\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = build_model(max_len=MAXLEN)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Finalizing the model\n",
    "Now that we successfully created our model, we have to finalize it by defining the loss that has to be minimized and which optimizier we want to use.\n",
    "\n",
    "The Tensorflow documentation lists all available [losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses) and [optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) you should choose from.\n",
    "\n",
    "**NOTE**: This is a binary classification problem. Think about the correct loss function for this kind of problem.\n",
    "\n",
    "**NOTE**: Make sure to initialize your optimizer with the learning rate that was defined above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##########################\n",
    "## YOUR CODE HERE START ##\n",
    "##########################\n",
    "\n",
    "# Create an instance of an Adam optimizer with the learning rate defined above \n",
    "# Define the loss to be minimized in this binary classification problem\n",
    "\n",
    "optimizer = ....\n",
    "loss = ....\n",
    "\n",
    "model.compile(optimizer, loss=loss, metrics=[\"accuracy\"], jit_compile=True)\n",
    "\n",
    "##########################\n",
    "## YOUR CODE HERE END ##\n",
    "##########################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xuIZf3cCBujl"
   },
   "source": [
    "##########################\n",
    "## YOUR CODE HERE START ##\n",
    "##########################\n",
    "\n",
    "# Create an instance of an Adam optimizer with the learning rate defined above \n",
    "# Define the loss to be minimized in this binary classification problem\n",
    "\n",
    "optimizer = ....\n",
    "loss = ....\n",
    "\n",
    "model.compile(optimizer, loss=loss, metrics=[\"accuracy\"], jit_compile=True)\n",
    "\n",
    "##########################\n",
    "## YOUR CODE HERE END ##\n",
    "##########################"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oY6jN_P1Bu8H"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ugv-KOmQ43Sj"
   },
   "source": [
    "## Model training\n",
    "Now we are ready to fine-tune the BERT model on our dataset\n",
    "We will be monitoring the training process using tensorboard. Once the training is launche you will be able to inspect logged metrics (under scalars) and track the the progress. \n",
    "\n",
    "In the graph section you can look at a visual representation of your model. Can you find your classification stack in the graph?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7Yi3JXlq6Nbm"
   },
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3zwM97pH3euQ"
   },
   "source": [
    "hist = model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=int(np.floor((len(train_ids) / BATCH_SIZE))),\n",
    "    validation_data=test_dataset,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "               tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", verbose=1, patience=1, restore_best_weights=True),\n",
    "               tf.keras.callbacks.TensorBoard(f'logs/{datetime.now()}')\n",
    "               ],\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fYNsaA55qY3"
   },
   "source": [
    "### Training metrics\n",
    "Lets inspect the training history to see how the losses and metrics behaved during training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1ztfWMpM38d3"
   },
   "source": [
    "history = pd.DataFrame({'epoch': hist.epoch, **hist.history}).set_index('epoch')\n",
    "history"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DPtyt7aE5DU4"
   },
   "source": [
    "history.plot()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ok2DISYt54kd"
   },
   "source": [
    "## Model evaluation\n",
    "Lets have a look how well the model is preforming on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7xaNvHGq5u9F"
   },
   "source": [
    "predictions = model.predict(test_dataset, batch_size=BATCH_SIZE, verbose=2, use_multiprocessing=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u7fDLp1SazB5"
   },
   "source": [
    "report = metrics.classification_report(y_true=test_labels, \n",
    "                                       y_pred=(predictions > 0.5).astype(int))\n",
    "print(report)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KrT0kEexbp_I"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C2nhDrmp97-n"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}